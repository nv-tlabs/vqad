
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 36px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row {
    font-size: 20px;
}
.affil-row {
    font-size: 18px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.image-center {
    display: block;
    margin-left: auto;
    margin-right: auto;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: #666;
    text-align: center;
    margin-top: 8px;
    margin-bottom: 8px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}

.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}
.image-center {
    display: block;
    margin-left: auto;
    margin-right: auto;
}

</style>

<div class="topnav" id="myTopnav">
  <a href="https://www.nvidia.com/"><img width="100%" src="assets/nvidia.svg"></a>
  <a href="https://www.nvidia.com/en-us/research/" ><strong>NVIDIA Research</strong></a>
  <a href="https://nv-tlabs.github.io/" ><strong>Toronto AI Lab</strong></a>
</div>

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title>Variable Bitrate Neural Fields</title>
        <meta property="og:description" content="Variable Bitrate Neural Fields"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <link rel="shortcut icon" type="image/x-icon" href="./favicon.ico?">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="@yongyuanxi">
        <meta name="twitter:title" content="Variable Bitrate Neural Fields">
        <meta name="twitter:description" content="
        Neural approximations of scalar- and vector fields, 
        such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations.
        State-of-the-art results are obtained by conditioning a neural approximation 
        with a lookup from trainable feature grids that take on part of the learning task and allow for smaller, 
        more efficient neural networks.
        Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption 
        compared to stand-alone neural network models.
        We present a dictionary method for compressing such feature grids, reducing their memory consumption 
        by up to 100 times and permitting a multiresolution representation which can be useful for out-of-core streaming.
        We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us 
        learn end-to-end discrete neural representations in a space where no direct supervision is available and 
        with dynamic topology and structure. 
">
        <meta name="twitter:image" content="https://nv-tlabs.github.io/vqad/assets/demo.jpg">

        <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-2DCMJB74G1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-2DCMJB74G1');
</script>

    </head>

 <body>
<div class="container">
    <div class="paper-title">
      <h1>Variable Bitrate Neural Fields</h1>
    </div>

    <div id="authors">
        <div class="author-row">
            <div class="col-4 text-center"><a href="https://tovacinni.github.io">Towaki Takikawa</a><sup>1,2</sup></div>
            <div class="col-4 text-center"><a href="https://research.nvidia.com/person/alex-evans">Alex Evans</a><sup>1</sup></div>
            <div class="col-4 text-center"><a href="https://research.nvidia.com/person/jonathan-tremblay">Jonathan Tremblay</a><sup>1</sup></div>
            <div class="col-4 text-center"><a href="https://tom94.net/">Thomas Müller</a><sup>1</sup></div><br>
            <div class="col-3 text-center"><a href="https://casual-effects.com/">Morgan McGuire</a><sup>3,4</sup></div>
            <div class="col-3 text-center"><a href="https://www.cs.toronto.edu/~jacobson/">Alec Jacobson</a><sup>2,5</sup></div>
            <div class="col-3 text-center"><a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a><sup>1,2</sup></div>
        </div>

        <div class="affil-row">
            <div class="col-5 text-center"><sup>1</sup>NVIDIA</a></div>
            <div class="col-5 text-center"><sup>2</sup>University of Toronto</div>
            <div class="col-5 text-center"><sup>3</sup>ROBLOX</div>
            <div class="col-5 text-center"><sup>4</sup>University of Waterloo</div>
            <div class="col-5 text-center"><sup>5</sup>Adobe Research</div>
        </div>
        <br>
        <div class="author-row">
            <div class="venue text-center"><b>SIGGRAPH 2022</b></div>
        </div>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://drive.google.com/file/d/1GTFPwQ3oe0etRJKP35oyRhHpsydTE_AR/view">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="paper-btn" href="https://github.com/nv-tlabs/vqad">
                <span class="material-icons"> code </span> 
                 Code
            </a>
            <a class="paper-btn" href="https://www.youtube.com/watch?v=Lh0CoTRNFBA">
                <span class="material-icons"> movie </span> 
                 Video
            </a>
        </div></div>
    </div>

    <section id="teaser">
        <img width="100%" src="assets/demo.jpg">
        <p class="caption">
            Using our vector-quantized auto-decoder (VQ-AD) method, 
  we compactly encode a 3D signal in a hierarchical representation which 
  can be used for progressive streaming and level of detail (LOD). 
  Two example neural radiance fields are shown after streaming from 5 to 8 levels of their 
  underlying octrees. The sizes shown are the total bytes streamed; that is, the finer LODs 
  include the cost of the coarser ones. Prior work such as NeRF requires approximately 2.5 MB
to be transferred before anything can be drawn.
        </p>
    </section>

    <section id="teaser-videos">
        <figure style="width: 33%; float: left;">
            <p style="text-align: center;">
            <iframe src="https://drive.google.com/file/d/1GE662f18l-t55-TsAmHRuBGQvXSf2hKl/preview"
                width="85%"
                allowtransparency="true" frameborder="0" scrolling="no" 
                allow="autoplay" allowfullscreen="true">Your browser doesn't support iFrames.</iframe>
            </p>
            <p class="caption">
            Compressing the Notre Dame model at different levels of details.
            </p>
        </figure>

        <figure style="width: 33%; float: left">
            <p style="text-align: center;">
            <iframe src="https://drive.google.com/file/d/17aIA9EbWGq6n9vTyITq0BdFm-40m4Pza/preview" 
                width="85%" 
                allowtransparency="true" frameborder="0" scrolling="no" 
                allow="autoplay" allowfullscreen="true">Your browser doesn't support iFrames.</iframe>
            <p class="caption">
            30-second fast forward video.
            </p>
        </figure>

        <figure style="width: 33%; float: left">
            <p style="text-align: center;">
            <iframe src="https://drive.google.com/file/d/1vOMsXf7iTho6ZI-M_NP7_XIY57MlNT4y/preview" 
                width="85%"
                allowtransparency="true" frameborder="0" scrolling="no" 
                allow="autoplay" allowfullscreen="true">Your browser doesn't support iFrames.</iframe>
            </p>
            <p class="caption">
            Compressing the Sakura model at different levels of details.
            </p>
        </figure>
    </section>
    
    <section id="abstract">
        <h2>Abstract</h2>
        <hr>
    <p>
    Neural approximations of scalar and vector fields, such as signed distance functions and radiance fields, have emerged as accurate, high-quality representations. State-of-the-art results are obtained by conditioning a neural approximation with a lookup from trainable feature grids that take on part of the learning task and allow for smaller, more efficient neural networks. Unfortunately, these feature grids usually come at the cost of significantly increased memory consumption compared to stand-alone neural network models. We present a dictionary method for compressing such feature grids, reducing their memory consumption by up to 100x and permitting a multiresolution representation which can be useful for out-of-core streaming. We formulate the dictionary optimization as a vector-quantized auto-decoder problem which lets us learn end-to-end discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure. Our source code will be available at https://github.com/nv-tlabs/vqad.
    </p>
    </section>

    <section id="news">
        <h2>News</h2>
        <hr>
        <div class="row">
            <div><span class="material-icons"> event </span> [May 2022] Our paper was accepted at SIGGRAPH 2022!</a>!</div>
        </div>
    </section>

    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="https://drive.google.com/file/d/1GTFPwQ3oe0etRJKP35oyRhHpsydTE_AR/view"><img class="screenshot" src="assets/paper.jpg"></a>
            </div>
            <div style="width: 50%">
                <p><b>Variable Bitrate Neural Fields</b></p>
                <p>Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas Müller, Morgan McGuire, Alec Jacobson, Sanja Fidler</p>
                <div><span class="material-icons"> description </span><a href="https://drive.google.com/file/d/1GTFPwQ3oe0etRJKP35oyRhHpsydTE_AR/view"> Paper (Uncompressed, 24.4MB) </a></div>
                <div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2206.07707"> arXiv (Compressed, 2.7MB) </a></div>
                <div><span class="material-icons"> insert_comment </span><a href="assets/sig.bib"> BibTeX</a></div>
                <div><span class="material-icons"> integration_instructions </span><a href="https://github.com/nv-tlabs/vqad"> Code</a></div>

                <p>Please send feedback and questions to <a href="https://tovacinni.github.io">Towaki Takikawa</a>.</p>
            </div>
        </div>
    </section>


    <section id="figures">
        <h2>Figures</h2>
        <hr>
        <p>
        You can download the uncompressed figures as a zip (23.7 MB)
        <a href="https://drive.google.com/file/d/1EPqowLUU4sjli_jbLeERycaSVpMEiAG1/view?usp=sharing">here</a>.
        </p>
        <p></p>
        <p></p>
        <p></p>
        <p></p>
        <img width="50%" src="assets/fig2.jpg" class="image-center">
        <p class="caption">
 Top-left shows a baseline neural radiance field whose uncompressed feature grid weighs 15,207 kB. Our method, shown bottom right, compresses this by a factor of 60x, with minimal visual impact (PSNR shown relative to training images). In a streaming setting, a coarse LOD can be displayed after receiving only the first kB of data. All sizes are without any additional entropy encoding of the bit-stream.
        </p>
        <img width="100%" src="assets/fig3.jpg" class="image-center">
        <p class="caption">
        (a) shows the baseline uncompressed version of our data structure, in which we store the bulky feature vectors at every grid vertex, of which there may be millions. In (b), we store a compact b-bit code per vertex, which indexes into a small codebook of feature vectors. 
        This reduces the total storage size, and this representation is directly used at inference time. 
        This indexing operation is not differentiable; at training time (c), we replace the indices with vectors C_i of width 2^b, to which softmax is applied before multiplying with the entire codebook. 
        This 'soft-indexing' operation is differentiable, and can be converted back to 'hard' indices used in (b) through an argmax operation.
        </p>
        <img width="80%" src="assets/fig4.jpg" class="image-center">
        <p class="caption">
        Post-Process vs. Learned Vector Quantization. We compare applying k-means vector quantization on the feature grid as a post-processing after training, vs. learning vector quantization end-to-end with the same number of codebook entries. We see the k-means quantization has visible discoloration, whereas ours preserves the visual quality.
        </p>
        <img width="70%" src="assets/fig5.jpg" class="image-center">
        <p class="caption">
        Compressing geometry. We show how VQ-AD can compress signed distance functions as in NGLOD. 
  Our method introduces visible artifacts in the normals, however it does result in a significant bitrate reduction. 
  We also compare against a quantized Draco mesh which has similar bitrates when entropy coded 
  2 MB as the decompressed binary .ply mesh).
        </p>
        <img width="70%" src="assets/fig6.jpg" class="image-center">
        <p class="caption">
        Qualitative comparison of static and learned indices. We qualitatively compare a hash approach with 12 bitwidth codebooks and our learned indices with 4 bitwidth codebooks which have similar compression rates. We see that our learned indices are able to reconstruct with less noise.
        </p>
        <img width="100%" src="assets/fig7.jpg" class="image-center">
        <p class="caption">
        Compressed levels of detail. From left to right: the different mip levels. Top row: mip-NeRF at different cone widths. Although mip-NeRF produces filtered results, they are constant bitrate. Bottom row: Our multiresolution and vector quantized representation. We are able to simultaneously filter and compress the representation, making it suitable for progressive streaming and level of detail.
        </p>
        <img width="60%" src="assets/fig8.jpg" class="image-center">
        <p class="caption">
        Rate Distortion Curve. This graph shows the rate-distortion tradeoffs of different methods on the 'Night Fury' RTMV scene, where the y-axis is PSNR and the x-axis is bitrate (in log-scale).
  Single-bitrate architectures are represented with a dot. For Mip-NeRF (purple), the filtering mechanism can move the dot vertically, but not horizontally.
  Our compressed architecture (red and blue) has variable-bitrate and is able to dynamically scale the bitrate to different levels of details.
  Our architecture is more compact than feature-grid methods like NGLOD (yellow) and achieves better quality than postprocessing methods like k-means VQ (gray and green).
        </p>
    </section>
    
    <section id="tables">
        <h2>Tables</h2>
        <hr>
        <p>
        <img width="50%" src="assets/table1.jpg" class="image-center">
        <p class="caption">
        Baseline References. This table shows the baseline feature-grid method (NGLOD-NeRF) in comparison to NeRF and mip-NeRF which are state-of-the-art global-methods, and Plenoxels which is also a feature-grid method. We see from the results that NGLOD-NeRF is a strong baseline with similar quality to both. All floats are half precision.
        </p>
        <img width="50%" src="assets/table2.jpg" class="image-center">
        <p class="caption">
        LRA, VQ vs loss-aware VQ (ours).
This table shows the comparison between low-rank approximation (LRA), vector quantization (kmVQ) and learned vector quantization (ours) at different truncation sizes (for LRA) and different quantization bitwidths (for kmVQ and ours).
We see that across all metrics we see a significant improvement by learning vector quantization. The bitrate is data dependent, so we report average bitrate.
        </p>
        <img width="50%" src="assets/table3.jpg" class="image-center">
        <p class="caption">
        Comparison between random indices and learned indices}. This table shows the effects of learning codebook indices with VQAD at 120 epochs with different quantization bitwidths (bw). To highlight the tradeoff, we list the size of the indices V and codebook D separately. We see that even when storing indices, we are able to achieve higher quality than the hash-based approach.
        </p>
    </section>

    <section id="acknowledgements">
        <h2>Acknowledgements</h2>
        <hr>
        <div class="row">
            <p>We would like to thank Joey Litalien, David Luebke, Or Perel, Clement Fuji-Tsang, and Charles Loop 
                for a whole lot of fruitful discussion for this project. 
                We would also like to thank Alexander Zook, Koki
Nagano, Jonathan Grasnkog, and Stan Birchfield for their help with
reviewing the draft for this paper.
            This website is mostly derived from the website for <a href="https://nv-tlabs.github.io/nglod/">NGLOD</a>, 
            which Joey Litalien hugely contributed to.
            </p>
        </div>
    </section>
</div>
</body>
</html>

